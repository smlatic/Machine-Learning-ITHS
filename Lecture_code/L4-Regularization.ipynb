{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kokchun/Machine-learning-AI22/blob/main/Lectures/L4-Regularization.ipynb\" target=\"_parent\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; for interacting with the code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Lecture notes - Regularized linear models\n",
    "---\n",
    "\n",
    "This is the lecture note for **regularized linear models**\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> that this lecture note gives a brief introduction to regularized linear models. I encourage you to read further about regularized linear models. </p>\n",
    "\n",
    "Read more:\n",
    "\n",
    "- [Regularized linear models medium](https://medium.com/analytics-vidhya/regularized-linear-models-in-machine-learning-d2a01a26a46)\n",
    "- [Ridge regression wikipedia](https://en.wikipedia.org/wiki/Ridge_regression)\n",
    "- [Tikhonov regularization wikipedia](https://en.wikipedia.org/wiki/Tikhonov_regularization)\n",
    "- [Lasso regression wikipedia](https://en.wikipedia.org/wiki/Lasso_(statistics))\n",
    "- [Korsvalidering](https://sv.wikipedia.org/wiki/Korsvalidering)\n",
    "- [Cross validation](https://machinelearningmastery.com/k-fold-cross-validation/)\n",
    "- [Scoring parameter sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [ISLRv2 pp 198-205](https://www.statlearning.com/)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.style.use(\"seaborn-white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134, 19), (66, 19), (134,), (66,))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "df = pd.read_csv(\"../data/Advertising.csv\", index_col=0)\n",
    "X, y = df.drop(\"Sales\", axis = 1), df[\"Sales\"]\n",
    "\n",
    "# in exercise 2 Polynomial regression you've found the elbow in degree 4, as the error increases after that\n",
    "# however to be safe and we assume that the model shouldn't have too many interactions between different features, I will choose 3\n",
    "# please try with 4 and see how your evaluation score differs\n",
    "model_polynomial = PolynomialFeatures(3, include_bias=False)\n",
    "poly_features = model_polynomial.fit_transform(X)\n",
    "\n",
    "# important to not forget \n",
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# from 3 features we've featured engineered 34\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature standardization\n",
    "Remove sample mean and divide by sample standard deviation \n",
    "\n",
    "$X' = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "LASSO, Ridge and Elasticnet regression that we'll use later require that the data is scaled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled X_train mean -0.00, std 1.00\n",
      "Scaled X_test mean -0.12, std 1.12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaled X_train mean {scaled_X_train.mean():.2f}, std {scaled_X_train.std():.2f}\")\n",
    "print(f\"Scaled X_test mean {scaled_X_test.mean():.2f}, std {scaled_X_test.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Regularization techniques\n",
    "\n",
    "Problem with overfitting was discussed in previous lecture. When model is too complex, data noisy and dataset is too small the model picks up patterns in the noise. The output of a linear regression is the weighted sum: \n",
    "$y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\ldots + \\theta_nx_n$ , where the weights $\\theta_i$ represents the importance of the $ith$ feature. Want to constrain the weights associated with noise, through regularization. We do this by adding a regularization term to the cost function used in training the model. Note that the cost function for evaluation now will differ from the training.\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> most regularization model requires scaling of data </p>\n",
    "\n",
    "---\n",
    "### Ridge regression \n",
    "Also called Tikhonov regularization or $\\ell_2$ regularization.\n",
    "\n",
    "$C(\\vec{\\theta}) = MSE(\\vec{\\theta}) + \\lambda \\frac{1}{2}\\sum_{i=1}^n \\theta_i^2$\n",
    "\n",
    "where $\\lambda \\ge 0$ is the ridge parameter or the penalty term, which reduces variance by increasing bias. Observe that the sum starts from 1, so the bias term $\\theta_0$ is not affected by $\\lambda$. Therefore by the larger the $\\lambda$ the more $\\theta_i, i = {1,2,\\ldots}$ causes higher error. As variance is decreasing and bias increasing, the model fits worse to the training datas noise and generalizes better.\n",
    "\n",
    "From the closed form OLS solution to ridge regression, we see that $\\lambda = 0$ gives us the normal equation for linear regression: \n",
    "\n",
    "$\\hat{\\vec{\\theta}} = (X^TX + \\lambda I)^{-1}X^T\\vec{y}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5148267621786812, 0.37485164412180333)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def ridge_regression(X, penalty=0):\n",
    "    # alpha = 0 should give linear regression\n",
    "    # note that alhpa is same as lambda in theory, i.e. penalty term. sklearn has chosen alpha to generalize their API\n",
    "    model_ridge = Ridge(alpha=penalty) \n",
    "    model_ridge.fit(scaled_X_train, y_train)\n",
    "    y_pred = model_ridge.predict(X)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "y_pred = ridge_regression(scaled_X_test, 0)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "RMSE, mean_absolute_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5148267621786576, 0.3748516441217811)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with linear regression -> RMSE very similar!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(scaled_X_train, y_train)\n",
    "y_pred = model_linear.predict(scaled_X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lasso regression\n",
    "\n",
    "Lasso - Least Absolute Shrinkage and Selection Operator or $\\ell_1$ regularization. Cost function for Lasso is: \n",
    "\n",
    "$C(\\vec{\\theta}) = MSE(\\vec{\\theta}) + \\lambda\\sum_{i=1}^n |\\theta_i|$\n",
    "\n",
    "It sets least important features to zero, when $\\lambda$ sufficiently large. This is practically feature selection. \n",
    "\n",
    "Note that in Lasso regression it is important to have scaled your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7853962108799017, 0.5735346450114956)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model_lasso = Lasso(alpha = .1)\n",
    "model_lasso.fit(scaled_X_train, y_train)\n",
    "y_pred = model_lasso.predict(scaled_X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross-validation\n",
    "\n",
    "One strategy to choose the best hyperparameter alpha is to take the training part of the data and \n",
    "1. shuffle dataset randomly\n",
    "2. split into k groups\n",
    "3. for each group -> take one test, the rest training -> fit the model -> predict on test -> get evaluation metric\n",
    "4. take the mean of the evaluation metrics\n",
    "5. choose the parameters and train on the entire training dataset\n",
    "\n",
    "Repeat this process for each alpha, to see which yielded lowest RMSE. k-fold cross-validation: \n",
    "- good for smaller datasets\n",
    "- fair evaluation, as a mean of the evaluation metric for all k groups is calculated\n",
    "- expensive to compute as it requires k+1 times of training\n",
    "\n",
    "---\n",
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV # ridge regression with cross-validation\n",
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "#print(SCORERS.keys())\n",
    "# negative because sklearn uses convention of higher return values are better\n",
    "model_ridgeCV = RidgeCV(alphas = [.0001, .001, .01, .1, .5, 1, 5, 10], scoring = \"neg_mean_squared_error\")\n",
    "model_ridgeCV.fit(scaled_X_train, y_train)\n",
    "print(model_ridgeCV.alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5635899169556632, 0.4343075766484079)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best alpha is 0.1\n",
    "# it seems that linear regression outperformed ridge regression in this case\n",
    "# however it could depend on the distribution of the train|test data, so using alpha = 0.1 is more robust here\n",
    "y_pred = model_ridgeCV.predict(scaled_X_test)\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "RMSE, mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.84681185,  0.52142086,  0.71689997, -6.17948738,  3.75034058,\n",
       "       -1.36283352, -0.08571128,  0.08322815, -0.34893776,  2.16952446,\n",
       "       -0.47840838,  0.68527348,  0.63080799, -0.5950065 ,  0.61661989,\n",
       "       -0.31335495,  0.36499629,  0.03328145, -0.13652471])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ridgeCV.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.004968802520343366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5785146895301977, 0.46291883026932984)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# it is trying 100 different alphas along regularization path epsilon\n",
    "model_lassoCV = LassoCV(eps = 0.001, n_alphas = 100, max_iter=1e4, cv=5)\n",
    "model_lassoCV.fit(scaled_X_train, y_train)\n",
    "print(f\"alpha = {model_lassoCV.alpha_}\")\n",
    "\n",
    "y_pred = model_lassoCV.predict(scaled_X_test)\n",
    "\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.19612354,  0.43037087,  0.29876351, -4.80417579,  3.46665205,\n",
       "       -0.40507212,  0.        ,  0.        ,  0.        ,  1.35260206,\n",
       "       -0.        ,  0.        ,  0.14879719, -0.        ,  0.        ,\n",
       "        0.        ,  0.09649665,  0.        ,  0.04353956])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we notice that many coefficients have been set to 0 using Lasso\n",
    "# it has selected some features for us \n",
    "model_lassoCV.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net\n",
    "\n",
    "Elastic net is a combination of both Ridge l2-regularization and Lasso l1-regularization. The cost function to be minimized for elastic net is: \n",
    "\n",
    "$$C(\\vec{\\theta}) = MSE(\\vec{\\theta}) + \\lambda\\left(\\alpha\\sum_{i=1}^n |\\theta_i| + \\frac{1-\\alpha}{2}\\sum_{i=1}^n \\theta_i^2\\right)$$\n",
    "\n",
    ", where $\\alpha$ here determines the ratio for $\\ell_1$ or $\\ell_2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 ratio: 1.0\n",
      "alpha 0.004968802520343366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# note that alpha here is lambda in the theory\n",
    "# l1_ratio is alpha in the theory\n",
    "model_elastic = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], eps = 0.001, n_alphas = 100, max_iter=1e4)\n",
    "model_elastic.fit(scaled_X_train, y_train)\n",
    "print(f\"L1 ratio: {model_elastic.l1_ratio_}\") # this would remove ridge and pick Lasso regression entirely\n",
    "print(f\"alpha {model_elastic.alpha_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5785146895301977, 0.46291883026932984)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_elastic.predict(scaled_X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)\n",
    "# note that the result is same for Lasso regression which is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Kokchun Giang\n",
    "\n",
    "[LinkedIn][linkedIn_kokchun]\n",
    "\n",
    "[GitHub portfolio][github_portfolio]\n",
    "\n",
    "[linkedIn_kokchun]: https://www.linkedin.com/in/kokchungiang/\n",
    "[github_portfolio]: https://github.com/kokchun/Portfolio-Kokchun-Giang\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99284c818dbeb598aa1f68004bbb0b5edd120a68b4d8720a3a3dd099e220abe1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('ML_exploration-5IpxFwVQ': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
